#!/usr/bin/python

"""
Script used to extract only the needed information from JSON packet traces generated by
tshark from PCAPNG format
"""

#  This file is part of NoMoATS <http://athinagroup.eng.uci.edu/projects/nomoads/>.
#  Copyright (C) 2019 Anastasia Shuba.
#
#  NoMoATS is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  NoMoATS is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with NoMoATS.  If not, see <http://www.gnu.org/licenses/>.

import os, sys
import json
import uuid
import argparse

from sets import Set
from urlparse import urlsplit
from collections import OrderedDict

from pii_helper import PIIHelper
import json_keys

# Prepare PII helper
pii_helper = PIIHelper(json_keys.PII_VALUES, json_keys.LOCATION_PII)

def make_unique(key, dct):
    counter = 0
    unique_key = key

    while unique_key in dct:
        counter += 1
        unique_key = key + "_" + str(counter)
    return unique_key


def parse_object_pairs(pairs):
    dct = OrderedDict()
    for key, value in pairs:
        if key in dct:
            key = make_unique(key, dct)
        dct[key] = value

    return dct

def check_ats(trace, ats_pkgs, packet):
    for pkg in ats_pkgs:
        if pkg in trace:
            #print pkg + " found in trace:"
            #print trace
            packet[json_keys.ats_label] = 1
            packet["ats_pkg"] = pkg
            return
            
    # TODO: check and save native func usage. (Can look for Native Method, but difficult to diff...)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TunNativeInterface.pollRead(Native Method)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TunNativeInterface.pollRead(TunNativeInterface.java:81)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TUNReader.run(TUNReader.java:65)
    #     at java.lang.Thread.run(Thread.java:761)

    packet[json_keys.ats_label] = 0

def extract_from_webview(full_path, ats_pkgs, data, pkg_name):
    with open(full_path, "r") as jf:
        webview_data = json.load(jf)
        
    for key in webview_data:
        new_packet = webview_data[key]

        if json_keys.trace not in new_packet:
            print "\tWARNING: no trace found"
            continue
        
        # Parse URL into ReCon format
        url_key = "url"
        url = new_packet[url_key]
        del new_packet[url_key]
        url_parse = urlsplit(url)
        if url_parse.port is None:
            if url_parse.scheme == json_keys.http:
                port = 80
            elif url_parse.scheme == "https":
                port = 443
            else:
                print "WARNING: unknown schema: " + url
                port = -1
        else:
            port = url_parse.port
        
        uri = url_parse.path
        if url_parse.query != '':
            uri += "?" + url_parse.query
        if url_parse.fragment != '':
            uri += "#" + url_parse.fragment
            
        new_packet[json_keys.dst_port] = port
        new_packet[json_keys.host] = url_parse.netloc
        new_packet[json_keys.uri] = uri
        new_packet[json_keys.type] = "webview"
        new_packet[json_keys.package_name] = pkg_name
        check_ats(new_packet[json_keys.trace], ats_pkgs, new_packet)

        # Find PII
        redacted_packet, pii_found = pii_helper.get_pii_from_data(new_packet)
        redacted_packet[json_keys.pii_label] = pii_found
        
        data[key] = redacted_packet


def extract_from_tshark(full_path, ats_pkgs, data, pkg_name):
    with open(full_path, "r") as jf:
        # Since certain json 'keys' appear multiple times in our data, we have to make them
        # unique first (we can't use regular json.load() or we lose some data points). From:
        # https://stackoverflow.com/questions/29321677/python-json-parser-allow-duplicate-keys
        decoder = json.JSONDecoder(object_pairs_hook=parse_object_pairs)
        pcap_data = decoder.decode(jf.read())

        for packet in pcap_data:
            layers = packet[json_keys.source][json_keys.layers]

            # All captured traffic should have a frame + frame number, but check anyway
            frame_num = " Frame: "
            if json_keys.frame not in layers or json_keys.frame_num not in layers[json_keys.frame]:
                print "WARNING: could not find frame number! Using -1..."
                frame_num = frame_num + "-1"
            else:
                # Save frame number for error-reporting
                frame_num = frame_num + layers[json_keys.frame][json_keys.frame_num]

            # All captured traffic should be IP, but check anyway
            if not json_keys.ip in layers:
                print "WARNING: Non-IP traffic detected!" + frame_num
                continue

            # For now, focus on HTTP only
            if json_keys.tcp not in layers or json_keys.http not in layers:
                continue

            # Fill our new JSON packet with TCP/IP info
            new_packet = {}
            new_packet[json_keys.dst_ip] = layers[json_keys.ip][json_keys.ip + ".dst"]
            new_packet[json_keys.dst_port] = int(layers[json_keys.tcp][json_keys.tcp + ".dstport"])

            # Extract and parse the packet comment
            if (json_keys.pkt_comment not in layers or
                        json_keys.frame_comment not in layers[json_keys.pkt_comment]):
                print "WARNING: no packet comment found!" + frame_num
                continue

            # TODO: how to label other third parties? Untraced pkts?
            comment = layers[json_keys.pkt_comment][json_keys.frame_comment]
            if "Thread name: Chrome_IOThread" in comment:
                # Skip webview packets as they are captured separately
                continue

            comment_lines = comment.split('\n', 1)
            # Save module name which is in the first line
            new_packet[json_keys.type] = comment_lines[0]
            # Save the rest of the stack trace
            new_packet[json_keys.trace] = comment_lines[1]

            check_ats(comment, ats_pkgs, new_packet)
            new_packet[json_keys.package_name] = pkg_name

            # Go through all HTTP fields and extract the ones that are needed
            http_data = layers[json_keys.http]
            for http_key in http_data:
                http_value = http_data[http_key]

                if http_key.startswith(json_keys.http_req_line):
                    header_line = http_value.split(":", 1)
                    if len(header_line) != 2:
                        print ("WARNING: could not parse header '" + str(header_line) + "'"
                               + frame_num)
                        continue

                    # Prepare container for HTTP headers
                    if json_keys.headers not in new_packet:
                        new_packet[json_keys.headers] = {}

                    # Use lower case for header keys to stay consistent with our other data
                    header_key = header_line[0].lower()

                    # Remove the trailing carriage return
                    header_val = header_line[1].strip()

                    # Save the header key-value pair
                    new_packet[json_keys.headers][header_key] = header_val

                    # If this is the host header, we also save it to the main object
                    if header_key == json_keys.host:
                        new_packet[json_keys.host] = header_val

                if json_keys.http_req_method in http_value:
                    new_packet[json_keys.method] = http_value[json_keys.http_req_method]
                if json_keys.http_req_uri in http_value:
                    new_packet[json_keys.uri] = http_value[json_keys.http_req_uri]

            # End of HTTP parsing

            # Check that we found the minimum needed HTTP headers
            if (json_keys.uri not in new_packet or json_keys.method not in new_packet or
                    json_keys.host not in new_packet):
                #print "Missing some HTTP Headers!" + frame_num
                continue

            # Extract timestamp
            if json_keys.frame_ts not in layers[json_keys.frame]:
                print "WARNING: could not find timestamp!" + frame_num
                continue

            new_packet["ts"] = layers[json_keys.frame][json_keys.frame_ts]

            # Find PII
            redacted_packet, pii_found = pii_helper.get_pii_from_data(new_packet)
            redacted_packet[json_keys.pii_label] = pii_found

            # Create a unique key for each packet to keep consistent with ReCon
            # Also good in case packets end up in different files
            data[str(uuid.uuid4())] = redacted_packet

def write_data(data, file_out):
    # Write the new data
    with open(file_out, "w") as jf:
        #print json.dumps(data, sort_keys=True, indent=4)
        jf.seek(0)
        jf.write(json.dumps(data, sort_keys=True, indent=4))
        jf.truncate()


def extract(tshark_file, webview_file, libadar_file, libradar_parser, out_file):
    """
    Extracts only the needed information from provided JSON packet traces and labels them
    :param tshark_file: JSON file containing data extracted via tshark
    :param webview_file: JSON file containing webview request data
    :param libadar_file: LibRadar or LibRadar++ analysis file
    :param libradar_parser: LibRadarParser instance
    :param out_file: File to write results to
    :return: True on success, False on failure
    """

    if not os.path.isfile(tshark_file):
        print "ERROR: invalid argument"
        return False

    ats_pkgs = libradar_parser.parse(libadar_file)

    # Prepare new data structure for re-formatted JSON storage
    data = {}
    fn = os.path.basename(libadar_file)
    pkg_name = fn[:len(fn) - len(".txt")]

    extract_from_tshark(tshark_file, ats_pkgs, data, pkg_name)
    if os.path.isfile(webview_file):
        extract_from_webview(webview_file, ats_pkgs, data, pkg_name)
    else:
        print "INFO: no webivew packets were captured"

    write_data(data, out_file)
    return True
