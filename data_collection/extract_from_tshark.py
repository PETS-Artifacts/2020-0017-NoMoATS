#!/usr/bin/python

"""
Script used to extract only the needed information from JSON packet traces generated by
tshark from PCAPNG format
"""

import os, sys
import json, csv
import uuid
import argparse

from sets import Set
from urlparse import urlsplit
from collections import OrderedDict

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + os.sep + "..")
from utils.pii_helper import PIIHelper
from utils import utils, json_keys

# Prepare PII helper
pii_helper = PIIHelper(json_keys.PII_VALUES, json_keys.LOCATION_PII)

ats_types = ["Advertisement", "Mobile Analytics"]

def make_unique(key, dct):
    counter = 0
    unique_key = key

    while unique_key in dct:
        counter += 1
        unique_key = key + "_" + str(counter)
    return unique_key


def parse_object_pairs(pairs):
    dct = OrderedDict()
    for key, value in pairs:
        if key in dct:
            key = make_unique(key, dct)
        dct[key] = value

    return dct

def check_ats(trace, ats_pkgs, packet):
    for pkg in ats_pkgs:
        if pkg in trace:
            #print pkg + " found in trace:"
            #print trace
            packet[json_keys.ats_label] = 1
            packet["ats_pkg"] = pkg
            return
            
    # TODO: check and save native func usage. (Can look for Native Method, but difficult to diff...)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TunNativeInterface.pollRead(Native Method)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TunNativeInterface.pollRead(TunNativeInterface.java:81)
    #     at edu.uci.calit2.antmonitor.lib.vpn.TUNReader.run(TUNReader.java:65)
    #     at java.lang.Thread.run(Thread.java:761)

    packet[json_keys.ats_label] = 0

# TODO: this should be run outside of this script to avoid running it for every file
def parse_literadar_csv(csv_file):
    all_ats_pkgs = Set()
    with open(csv_file, 'rb') as f:
        reader = csv.reader(f, delimiter=',')

        # Get headers
        header_row = reader.next()
        for row in reader:
            pkg = row[header_row.index("Package Name")]
            type = row[header_row.index("Type")]
            if type in ats_types:
                all_ats_pkgs.add(pkg)

    #print all_ats_pkgs
    print "Total packages: " + str(len(all_ats_pkgs))
    return all_ats_pkgs
            
def parse_literadar(literadar_file, global_ats_pkgs):
    """
    Parse LiteRadar data
    """
    with open(literadar_file) as jf:
        ats_libs = Set()

        pkg_name_in_apk_idx = 0
        pkg_name_orig_idx = 1

        for line in jf:
            split_line = line.split()
            pkg_name_in_apk = split_line[pkg_name_in_apk_idx]
            pkg_name_orig = split_line[pkg_name_orig_idx]

            if pkg_name_orig in global_ats_pkgs:
                # change Lcom/crashlytics/android to com.crashlytics.android
                pkg = pkg_name_in_apk[1:].replace("/", ".")
                print "\tFound ATS pkg: " + pkg
                ats_libs.add(pkg)
            # Special case for Google:
            elif pkg_name_orig == "Lcom/google/android/gms":
                # From https://developers.google.com/android/reference/packages
                ats_libs.add("com.google.android.gms.ads")
                ats_libs.add("com.google.android.gms.internal.ads")
                ats_libs.add("com.google.android.gms.analytics")

    print ats_libs
    return ats_libs

def extract_from_webview(full_path, ats_pkgs, data, pkg_name):
    with open(full_path, "r") as jf:
        webview_data = json.load(jf)
        
    for key in webview_data:
        new_packet = webview_data[key]

        if json_keys.trace not in new_packet:
            print "\tWARNING: no trace found"
            continue
        
        # Parse URL into ReCon format
        url_key = "url"
        url = new_packet[url_key]
        del new_packet[url_key]
        url_parse = urlsplit(url)
        if url_parse.port is None:
            if url_parse.scheme == json_keys.http:
                port = 80
            elif url_parse.scheme == "https":
                port = 443
            else:
                print "WARNING: unknown schema: " + url
                port = -1
        else:
            port = url_parse.port
        
        uri = url_parse.path
        if url_parse.query != '':
            uri += "?" + url_parse.query
        if url_parse.fragment != '':
            uri += "#" + url_parse.fragment
            
        new_packet[json_keys.dst_port] = port
        new_packet[json_keys.host] = url_parse.netloc
        new_packet[json_keys.uri] = uri
        new_packet[json_keys.type] = "webview"
        new_packet[json_keys.package_name] = pkg_name
        check_ats(new_packet[json_keys.trace], ats_pkgs, new_packet)

        # Find PII
        redacted_packet, pii_found = pii_helper.get_pii_from_data(new_packet)
        redacted_packet[json_keys.pii_label] = pii_found
        
        data[key] = redacted_packet


def extract_from_tshark(full_path, ats_pkgs, data, pkg_name):
    with open(full_path, "r") as jf:
        # Since certain json 'keys' appear multiple times in our data, we have to make them
        # unique first (we can't use regular json.load() or we lose some data points). From:
        # https://stackoverflow.com/questions/29321677/python-json-parser-allow-duplicate-keys
        decoder = json.JSONDecoder(object_pairs_hook=parse_object_pairs)
        pcap_data = decoder.decode(jf.read())

        for packet in pcap_data:
            layers = packet[json_keys.source][json_keys.layers]

            # All captured traffic should have a frame + frame number, but check anyway
            frame_num = " Frame: "
            if json_keys.frame not in layers or json_keys.frame_num not in layers[json_keys.frame]:
                print "WARNING: could not find frame number! Using -1..."
                frame_num = frame_num + "-1"
            else:
                # Save frame number for error-reporting
                frame_num = frame_num + layers[json_keys.frame][json_keys.frame_num]

            # All captured traffic should be IP, but check anyway
            if not json_keys.ip in layers:
                print "WARNING: Non-IP traffic detected!" + frame_num
                continue

            # For now, focus on HTTP only
            if json_keys.tcp not in layers or json_keys.http not in layers:
                continue

            # Fill our new JSON packet with TCP/IP info
            new_packet = {}
            new_packet[json_keys.dst_ip] = layers[json_keys.ip][json_keys.ip + ".dst"]
            new_packet[json_keys.dst_port] = int(layers[json_keys.tcp][json_keys.tcp + ".dstport"])

            # Extract and parse the packet comment
            if (json_keys.pkt_comment not in layers or
                        json_keys.frame_comment not in layers[json_keys.pkt_comment]):
                print "WARNING: no packet comment found!" + frame_num
                continue

            # TODO: how to label other third parties? Untraced pkts?
            comment = layers[json_keys.pkt_comment][json_keys.frame_comment]
            if "Thread name: Chrome_IOThread" in comment:
                # Skip webview packets as they are captured separately
                continue

            comment_lines = comment.split('\n', 1)
            # Save module name which is in the first line
            new_packet[json_keys.type] = comment_lines[0]
            # Save the rest of the stack trace
            new_packet[json_keys.trace] = comment_lines[1]

            check_ats(comment, ats_pkgs, new_packet)
            new_packet[json_keys.package_name] = pkg_name

            # Go through all HTTP fields and extract the ones that are needed
            http_data = layers[json_keys.http]
            for http_key in http_data:
                http_value = http_data[http_key]

                if http_key.startswith(json_keys.http_req_line):
                    header_line = http_value.split(":", 1)
                    if len(header_line) != 2:
                        print ("WARNING: could not parse header '" + str(header_line) + "'"
                               + frame_num)
                        continue

                    # Prepare container for HTTP headers
                    if json_keys.headers not in new_packet:
                        new_packet[json_keys.headers] = {}

                    # Use lower case for header keys to stay consistent with our other data
                    header_key = header_line[0].lower()

                    # Remove the trailing carriage return
                    header_val = header_line[1].strip()

                    # Save the header key-value pair
                    new_packet[json_keys.headers][header_key] = header_val

                    # If this is the host header, we also save it to the main object
                    if header_key == json_keys.host:
                        new_packet[json_keys.host] = header_val

                if json_keys.http_req_method in http_value:
                    new_packet[json_keys.method] = http_value[json_keys.http_req_method]
                if json_keys.http_req_uri in http_value:
                    new_packet[json_keys.uri] = http_value[json_keys.http_req_uri]

            # End of HTTP parsing

            # Check that we found the minimum needed HTTP headers
            if (json_keys.uri not in new_packet or json_keys.method not in new_packet or
                    json_keys.host not in new_packet):
                #print "Missing some HTTP Headers!" + frame_num
                continue

            # Extract timestamp
            if json_keys.frame_ts not in layers[json_keys.frame]:
                print "WARNING: could not find timestamp!" + frame_num
                continue

            new_packet["ts"] = layers[json_keys.frame][json_keys.frame_ts]

            # Find PII
            redacted_packet, pii_found = pii_helper.get_pii_from_data(new_packet)
            redacted_packet[json_keys.pii_label] = pii_found

            # Create a unique key for each packet to keep consistent with ReCon
            # Also good in case packets end up in different files
            data[str(uuid.uuid4())] = redacted_packet

def write_data(data, file_out):
    # Write the new data
    with open(file_out, "w") as jf:
        #print json.dumps(data, sort_keys=True, indent=4)
        jf.seek(0)
        jf.write(json.dumps(data, sort_keys=True, indent=4))
        jf.truncate()

if __name__ == '__main__':
    ap = argparse.ArgumentParser(description=
        "Extracts only the needed information from provided JSON packet traces")
    ap.add_argument('tshark_file', help='JSON file containing data extracted via tshark')
    ap.add_argument('webview_file', help='JSON file containing webview request data')
    ap.add_argument('literadar_file', help='LiteRadar analysis file')
    ap.add_argument('literadar_csv', help='LiteRadar CSV file containing tag rules')
    ap.add_argument('out_file', help='File to write results to')

    args = ap.parse_args()
    
    file_in = args.tshark_file
    if not os.path.isfile(file_in):
        print "ERROR: please provide a JSON file as a first argument"
        sys.exit(1)

    global_ats_pkgs = parse_literadar_csv(args.literadar_csv)
    ats_pkgs = parse_literadar(args.literadar_file, global_ats_pkgs)
        
    # Prepare new data structure for re-formatted JSON storage
    data = {}
    fn = os.path.basename(args.literadar_file)
    pkg_name = fn[:len(fn) - len(".txt")]
    
    extract_from_tshark(file_in, ats_pkgs, data, pkg_name)
    if os.path.isfile(args.webview_file):
        extract_from_webview(args.webview_file, ats_pkgs, data, pkg_name)
    else:
        print "INFO: no packets were captured"
    write_data(data, args.out_file)
